{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping Actor-Critic algorithms in LunarLander-v2 environment (A2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-inf, inf, (8,), float32)\n",
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "env = gym.make('LunarLander-v2')\n",
    "print(env.observation_space)\n",
    "print(env.action_space)bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "probs = np.array([0.5, 0.25, 0.25])\n",
    "np.random.choice(3, p = probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps:\n",
    "\n",
    "1. play n steps in the environment and save (state, action, next_state), default N = ?\n",
    "2. Initialize R = 0 or R = V(St)\n",
    "3. calculate loss = td_loss + policy_gradient + (entropy_loss)\n",
    "4. update params\n",
    "5. repeat\n",
    "\n",
    "\n",
    "# Programming steps:\n",
    "1. ExpSource step return discounted reward state and action, paramas = n_step, net\n",
    "2. function to transform list of experiences into batch of states actions and reward (non terminal state rewards summed with value net output)\n",
    "3. functions to calculate approriate losses\n",
    "4. take optimizer step\n",
    "5. incorporate logger into the mix\n",
    "6. try to implement experience source with multiple environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define experience source \n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from utils import Experience\n",
    "\n",
    "PolicyExperience = namedtuple('PolicyExperience', ('state', 'action', 'reward', 'isdone'))\n",
    "\n",
    "\"\"\"\n",
    "step returns experience with n_step unroll \n",
    "\n",
    "\"\"\"\n",
    "class SamplingPolicy:\n",
    "    \n",
    "    def __init__(self, net):\n",
    "        \n",
    "        self.net = net\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        get sampled action from state\n",
    "        currently supports only single action at a time\n",
    "        \"\"\"\n",
    "        \n",
    "        logits, _ = self.net(state)\n",
    "        output_dim = logits.shape[1]\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        print(probs)\n",
    "        return np.random.choice(output_dim, p=probs[0])\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "class ExperienceSourceForPolicy:\n",
    "    \n",
    "    def __init__(self, env, n_steps, gamma = 0.99, device=\"cpu\"):\n",
    "        \n",
    "        self.env = env\n",
    "        self.state = self.env.reset()\n",
    "        \n",
    "        self.episode_reward = 0\n",
    "        self.episode_steps = 0\n",
    "        \n",
    "        self.n_steps = n_steps\n",
    "        \n",
    "        self.device = device\n",
    "        self.steps_done = 0\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self, policy):\n",
    "        \n",
    "        state = self.state\n",
    "        obs_tens = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        act = policy.get_action(obs_tens)\n",
    "        \n",
    "        obs, reward, isdone = self.env.step(act)\n",
    "        \n",
    "        self.episode_steps +=1\n",
    "        self.episode_reward+=reward\n",
    "        \n",
    "        first_action = act\n",
    "        total_reward = reward\n",
    "        \n",
    "        if (not isdone):\n",
    "            for i in range(self.n_steps-1):\n",
    "                obs_tens = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "                act = policy.get_action(obs_tens)\n",
    "                obs, reward, isdone, _ = self.env.step(act)\n",
    "                total_reward+=(self.gamma**(i+1))*reward\n",
    "                self.episode_reward+=reward\n",
    "                self.episode_steps+=1\n",
    "                if isdone: \n",
    "                    break\n",
    "                    \n",
    "        exp = Experience(state, first_action, obs, total_reward, isdone)\n",
    "        \n",
    "        if isdone:\n",
    "            self.state=self.env.reset()\n",
    "            episode_reward = self.episode_reward\n",
    "            episode_steps = self.episode_steps\n",
    "            \n",
    "            self.episode_steps = 0\n",
    "            self.episode_reward = 0\n",
    "                \n",
    "            return exp, (episode_reward, episode_steps)\n",
    "        \n",
    "        \n",
    "        self.state = obs\n",
    "        \n",
    "        return exp, None\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25525242 0.23736769 0.25064224 0.2567377 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class A2CBasicNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_size = 256):\n",
    "        \n",
    "        super(A2CBasicNet, self).__init__()\n",
    "        self.base = nn.Sequential(nn.Linear(input_dim, hidden_size), \n",
    "                                  nn.ReLU(), nn.Linear(hidden_size, hidden_size),\n",
    "                                  nn.ReLU(), nn.Linear(hidden_size, hidden_size),\n",
    "                                  nn.ReLU()\n",
    "                                 )\n",
    "        \n",
    "        self.policy = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(), nn.Linear(hidden_size, output_dim)\n",
    "                                   )\n",
    "        self.value = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                   nn.ReLU(), nn.Linear(hidden_size, 1)\n",
    "                                  )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.base(input)\n",
    "        policy_logits = self.policy(x)\n",
    "        value = self.value(x)\n",
    "        return policy_logits, value\n",
    "    \n",
    "net = A2CBasicNet(8,4)\n",
    "policy = SamplingPolicy(net)\n",
    "policy.get_action(torch.randn(1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training loop (Params From Deep Reinforcement Learning Hands-On, Maxim Lapan)\n",
    "\n",
    "gamma = 0.99\n",
    "lr = 0.001\n",
    "beta = 0.01\n",
    "batch_size = 128\n",
    "num_envs = 50\n",
    "\n",
    "reward_Steps = 4\n",
    "clip_grad = 0.1\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "net = A2CBasicNet(8, 4)\n",
    "policy = SamplingPolicy(net)\n",
    "exp_source = ExperienceSourceForPolicy(env, n_steps = reward_steps)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
